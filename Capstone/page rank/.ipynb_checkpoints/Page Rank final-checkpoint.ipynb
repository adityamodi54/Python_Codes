{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: http://www.dr-chuck.com/\n",
      "['http://www.dr-chuck.com', 'www.dr-chuck.com']\n",
      "How many pages:1\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "# spider.py\n",
    "\n",
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            cur.execute('UPDATE Pages SET error=0 WHERE url=?', (url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations:10\n",
      "1 1.1330221394578733e-05\n",
      "2 8.16817026287886e-06\n",
      "3 5.313644938031992e-06\n",
      "4 3.191381665823556e-06\n",
      "5 1.8149655978572099e-06\n",
      "6 9.769982585061676e-07\n",
      "7 5.018304540360675e-07\n",
      "8 2.4329790373478486e-07\n",
      "9 1.1529675602028888e-07\n",
      "10 5.4211479305621424e-08\n",
      "[(1, 1.974569954535394), (5, 0.8526551947296421), (4, 0.8526551947296421), (2, 2.1989529442860407), (16, 0.8975317687979245)]\n"
     ]
    }
   ],
   "source": [
    "#  sprank.py\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1.974544525548033, 1.974569954535394, 1, 'http://www.dr-chuck.com')\n",
      "(4, 2.1989592662082194, 2.1989529442860407, 2, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')\n",
      "(2, 1.0097185826818749, 1.009723258701018, 10, 'http://www.dr-chuck.com/dr-chuck/resume/bio.htm')\n",
      "(2, 0.8975326922060487, 0.8975317687979245, 16, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')\n",
      "(2, 0.5609564185859218, 0.560957345863554, 18, 'http://www.dr-chuck.com/dr-chuck/resume')\n",
      "(1, 1.0, 1.0, 3, 'http://www.dr-chuck.com/office')\n",
      "(1, 0.8526682309274578, 0.8526551947296421, 4, 'http://www.dr-chuck.com/sakai-book')\n",
      "(1, 0.8526682309274578, 0.8526551947296421, 5, 'http://www.dr-chuck.com/obi-sample')\n",
      "(1, 0.5609564185859218, 0.560957345863554, 15, 'http://www.dr-chuck.com/dr-chuck/resume/leadership.htm')\n",
      "(1, 0.5609564185859218, 0.560957345863554, 17, 'http://www.dr-chuck.com/dr-chuck/resume/pictures/index.htm')\n",
      "(1, 0.5310392157431498, 0.5310396466296841, 27, 'http://www.dr-chuck.com/dr-chuck/resume/pictures')\n",
      "11 rows.\n"
     ]
    }
   ],
   "source": [
    "#  spdump \n",
    "\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 20\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "#  sp json\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  go yo spider.js\n",
    "# then force .js\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
